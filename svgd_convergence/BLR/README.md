# Hierarchical Bayesian Logistic Regression (BLR) Experiments

This directory contains a Hierarchical Bayesian Logistic Regression (BLR) model for binary classification using the UCI Covertype dataset, along with convergence experiments using the Stein Variational Gradient Descent (SVGD) algorithm.

The goal is to approximate the posterior distribution of the hierarchical BLR model's parameters using SVGD and evaluate its convergence against a "ground truth" posterior generated by MCMC.

## Model Structure

The hierarchical Bayesian model follows the structure described in the paper (Section 5.3):

- **Global precision parameter**: τ ~ Gamma(α_prior, β_prior)
- **Regression coefficients**: β_d ~ Normal(0, 1/τ) for d = 1, ..., D
- **Likelihood**: y_n ~ Bernoulli(logit(X_n^T β)) for n = 1, ..., N

This hierarchical structure allows the model to learn the appropriate regularization strength from the data.

## Dataset

- **Source**: UCI Covertype Dataset (binary version)
  - **Primary**: `covtype.libsvm.binary.scale.bz2` - Pre-scaled libsvm format binary dataset
  - **Fallback**: Original UCI Covertype dataset converted to binary classification
- **Features**: 54 dimensions
- **Classes**: 2 (binary classification)
- **Samples**: Full dataset used by default (libsvm format), or subset of 10,000 samples (fallback method)

The libsvm format dataset (`covtype.libsvm.binary.scale.bz2`) is the primary dataset used in the paper. This file contains pre-scaled binary classification data that matches the experimental setup described in the paper. The data is automatically loaded when available, with fallback to the original UCI Covertype dataset if the libsvm file is not found.

## Structure

- `blr_model.stan`: The Stan model definition for Hierarchical Bayesian Logistic Regression. It is used by `run_mcmc.py` to generate the reference posterior distribution via Hamiltonian Monte Carlo (HMC).
- `model.py`: A Python implementation of the hierarchical BLR model. It provides the necessary log-posterior gradient computations required by the SVGD algorithm.
- `SVGD.py`: A general-purpose implementation of the Stein Variational Gradient Descent (SVGD) algorithm, featuring an RBF kernel and the AdaGrad optimizer.
- `run_mcmc.py`: An execution script that uses `cmdstanpy` to run MCMC sampling on the `blr_model.stan`, producing the ground truth posterior parameters.
- `running_experiments_decay.py`: The main script for executing SVGD experiments. It utilizes the model from `model.py` and the algorithm from `SVGD.py` to approximate the posterior.
- `plot.ipynb`: A Jupyter notebook for visualizing and analyzing the experiment results, such as KL divergence and KSD convergence over iterations.
- `README.md`: This file.

## Setup

### Required Dependencies

```bash
pip install numpy pandas scikit-learn cmdstanpy tqdm matplotlib seaborn jupyter
```

### Installing CmdStan

CmdStan is a dependency for `cmdstanpy` and is required to run the MCMC sampling.

```bash
# CmdStanPy can automatically install the latest version of CmdStan
import cmdstanpy
cmdstanpy.install_cmdstan()
```

## Usage

The workflow consists of two main steps:

### 1. Generate True Posterior via MCMC

First, run MCMC using `cmdstanpy` to estimate the true posterior distribution parameters. This serves as the ground truth for evaluating SVGD.

```bash
python run_mcmc.py
```

This script performs the following:
- Loads and preprocesses the binary UCI Covertype dataset (libsvm format if available, fallback to original dataset).
- Compiles the Stan model defined in `blr_model.stan`.
- Executes MCMC sampling (default: 2000 samples, 1000 warmup, 4 chains).
- Saves the MCMC samples and calculated true posterior parameters (mean and covariance) to `mcmc_results_*.pkl`.

### 2. Execute SVGD Experiments

Once the MCMC results are available, run the SVGD experiments to approximate the posterior.

```bash
python running_experiments_decay.py
```

This script performs the following:
- Loads the true posterior parameters from the MCMC results file.
- Runs SVGD experiments for different numbers of particles (e.g., 5, 10, 20, 50).
- For each experiment, it computes KL divergence, KSD, and eigenvalue ratios against the true posterior.
- Saves the comprehensive results to `svgd_results_*.pkl`.

### Command Line Options

#### run_mcmc.py
```bash
# Example: Run with 2000 samples, 1000 warmup, 4 chains, and specific priors
python run_mcmc.py --n_samples 2000 --n_warmup 1000 --chains 4 --random_seed 42 --alpha_prior 1.0 --beta_prior 1.0
```

#### running_experiments_decay.py
```bash
# Example: Run with a specific number of iterations and priors
python running_experiments_decay.py --n_iterations 10000 --alpha_prior 1.0 --beta_prior 1.0
```

## Model and Algorithm

### Hierarchical Bayesian Logistic Regression

The posterior distribution is defined in two complementary ways:
1. **Stan Model (`blr_model.stan`)**: A formal probabilistic model definition used with the NUTS sampler in Stan to generate a high-fidelity reference posterior.
2. **Python Model (`model.py`)**: A Python implementation that provides the gradient of the log-posterior probability, which is required by the SVGD algorithm.

The hierarchical structure includes:
- A global precision parameter τ with a Gamma prior
- Regression coefficients β with a Normal prior that depends on τ
- Binary classification likelihood using the logistic link function

### Stein Variational Gradient Descent (SVGD)

SVGD is a particle-based variational inference method that iteratively updates a set of particles to approximate the target posterior distribution. The algorithm uses:
- **Kernel**: RBF kernel with adaptive bandwidth selection
- **Optimizer**: AdaGrad with momentum
- **Learning rate decay**: Configurable decay schedules for better convergence

## Evaluation Metrics

The experiments evaluate SVGD convergence using several metrics:

1. **KL Divergence**: Measures the difference between the SVGD particle distribution and the true posterior
2. **Kernel Stein Discrepancy (KSD)**: A kernel-based measure of distributional discrepancy
3. **Eigenvalue Analysis**: Examines the spectrum of the kernel matrix to understand the algorithm's behavior
4. **Accuracy**: Classification accuracy on test data using the learned parameters

## Results

Results are saved in the `results/` directory with the following naming convention:
- `mcmc_results_*.pkl`: MCMC posterior samples and parameters
- `svgd_results_n{particles}_beta{decay}_iter{iterations}.pkl`: SVGD experiment results

Each result file contains:
- Final particle positions
- Convergence metrics over iterations
- Model parameters and settings
- True posterior parameters for comparison

## Notes

- The libsvm format dataset (`covtype.libsvm.binary.scale.bz2`) is the primary dataset used in the paper and provides the most accurate reproduction of the experimental setup
- The binary classification setup provides a simpler but still challenging problem for evaluating SVGD convergence
- The hierarchical structure allows the model to adapt its regularization strength to the data
- MCMC convergence should be verified before using the results for SVGD evaluation
- The default hyperparameters are chosen to balance computational efficiency with accuracy
- The libsvm data is pre-scaled, so no additional standardization is applied

## Troubleshooting

### CmdStanPy Errors
If you encounter issues with CmdStan, try reinstalling it:
```bash
python -c "import cmdstanpy; cmdstanpy.install_cmdstan(overwrite=True, compiler=True)"
```

### Memory Issues
- Reduce the number of MCMC samples (e.g., `--n_samples 500`).
- Use a smaller subset of the data (e.g., by modifying `n_samples` in the data loading section of the scripts).

### Dependency Issues
Ensure all packages are up-to-date:
```bash
pip install --upgrade numpy pandas scikit-learn cmdstanpy tqdm matplotlib seaborn jupyter
``` 